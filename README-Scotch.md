Information on how to run SWIFT with Scotch mapping and the test environment used on Cosma 8. Code has been tested with Scotch version 7.0.2.

Last update 18th August 2023.


Obtaining Scotch (as per gitlab [repo](https://gitlab.inria.fr/scotch/scotch))
----------------

**Scotch** is publicly available under the CeCILL-C free software license, as described [here](https://gitlab.inria.fr/scotch/scotch/blob/master/LICENSE_en.txt). The license itself is available [here](https://gitlab.inria.fr/scotch/scotch/-/blob/master/doc/CeCILL-C_V1-en.txt).

To use the lastest version of **Scotch**, please clone the master branch:

    git clone git@gitlab.inria.fr:scotch/scotch.git

Tarballs of the **Scotch** releases are available [here](https://gitlab.inria.fr/scotch/scotch/-/releases).

Instructions for installing locally on Cosma 8
----------------
_Environment_
```
    module load cosma/2018 python/3.6.5 intel_comp/2022.1.2 compiler openmpi/4.1.1 fftw/3.3.9 parallel_hdf5/1.12.0 parmetis/4.0.3-64bit gsl/2.5
    module load cmake
    module load bison
```
Navigate to the Scotch directory and carry out the following commands

```
    mkdir build 
    cd build
    cmake -DCMAKE_INSTALL_PREFIX=/path-to-install-dir ..
    make -j5
    make install
```

Configure SWIFT with Scotch
----------------

Follow the usual installation [instructions](https://gitlab.cosma.dur.ac.uk/swift/swiftsim/-/blob/master/INSTALL.swift) but if Scotch installed locally the added `--with-scotch=\path-to-scotch` flag will need to be passed to `./configure`

Running with Scotch
----------------

Scotch carries out a _mapping_ of a _source_ (or process) graph onto a _target_ (or architecture) graph. The weighted _source_ graph is generated by SWIFT and it captures the computation and communication cost across the computational domain. The _target_ graph defines the communication cost across the available computing architecture. Therefore, to make use of the Scotch _mapping_ alogrithms a target architecture file (_target.tgt_) must be generated and it should mirror the set up of the cluster being used. Scotch provides optimised architecture files which capture most HPC set ups. For Cosma8 runs we will be targetting NUMA regions therefore we have modelled the architecture as a `tleaf` structure. 

In the following examples it is assumed that one mpi rank is mapped to each Cosma 8 NUMA region. This enforces that `cpus-per-task=16` is defined in the SLURM submission script. The Cosma 8 nodes consist of 8 NUMA regions per node, with 4 NUMA regions per socket. Example `tleaf` files for various setups are given below, where the intrasocket communication cost between NUMA regions is set at _5_, intranode but across sockets is set at _10_ and the internode cost is set at _1000_. These weightings are estimated values but have been shown to give satisfactory results in the testcases explored. An estimate of the relative latency between NUMA regions on a node can be obtained by using [hwloc](https://github.com/open-mpi/hwloc), specifically by using `hwloc-distances`.

**Example tleaf graphs to represent various Cosma8 configurations** 
| Number of nodes | Number of MPI ranks | tleaf                   |
| --------------- | ------------------- | ----------------------- |
| 1               | 2                   | tleaf 1 2 5             |
| 1               | 8                   | tleaf 2 2 10 4 5        |
| 2               | 16                  | tleaf 3 2 1000 2 10 4 5 |
| 4               | 32                  | tleaf 3 4 1000 2 10 4 5 |
| 8               | 64                  | tleaf 3 8 1000 2 10 4 5 |

The first index denotes the number of layers of the tleaf structure and the subsequent index pairs are the number of nodes in a layer and the cost to communicate between them. For example the numbers represented in the 64 MPI rank case (`tleaf 3 8 1000 2 10 4 5`) are as follows: reading left to right the `3` denotes the number of layers in the leaf structure, `8` denotes the number of vertices in the first layer (which corresponds to 8 compute nodes), `1000` the relative cost for internode communication, `2` denotes the number of vertices in the second layer (number of sockets on each node), `10` relative cost for intersocket communication, `4` denotes the number of vertices in the final layer (number of NUMA regions per socket on cosma8) and finally `5` is the relative cost of intrasocket communication.  

The user needs to define this tleaf structure and save it as `target.tgt` in the directory they will run SWIFT from. Ongoing work focuses on automatically generating this target architecture upon run time. 

With OpenMPI the `mpirun` option `--map-by numa` has been found to be optimal. This is in contrast to previously suggested `--bind-to none` on the cosma8 [site](https://www.dur.ac.uk/icc/cosma/support/cosma8/).

Scotch details
----------------

Scotch carries out the mapping using various strategies which are outlined in the [documentation](https://gitlab.inria.fr/scotch/scotch/-/blob/master/doc/scotch_user7.0.pdf), a list of the strategies trialed include: `SCOTCH_STRATDEFAULT`, `SCOTCH_STRATBALANCE`, `SCOTCH_STRATQUALITY` and `SCOTCH_STRATSPEED`. The Scotch strategy string is passed to the Mapping functions. For the runs carried out here it was found that the global flag `SCOTCH_STRATBALANCE` and an imbalance ratio of `0.05` worked best. These values are passed to [`SCOTCH_stratGraphMapBuild`](https://github.com/UCL/swiftsim/blob/cb06b0e5c3d8457c474d0084d973f437d29b20d8/src/partition.c#L1657). 

One issue with Scotch is that when the number of mpi ranks is comparable to the dimensionality of the modelled SWIFT system the optimal mapping strategy doesn't neccessarily map to all available NUMA regions. At present this isn't handled explicity in the code and the paritition reverts to a vectorised or previous partitioning.

The SWIFT edge and vertex weights are estimated in the code, however edge weights are not symmetric - this causes an issue with SWIFT. Therefore, in the SCOTCH Graph the edge weigths are updated to equal to the sum of the two associated edge weights as calculated from SWIFT.

Test Runs
----------------
The following results were obtained on cosma8 running with the `SCOTCH_STRATBALANCE` strategy string and an imbalance ratio of `0.05`.


| Testcase | Resources                   | flags           | Scotch (s) | Metis (s) |
| -------- | --------------------------- | --------------  | ---------- |  -------- |
| EAGLE_6  | nodes = 1 (8 NUMA regions)  | `--map_by numa` | 1307.8     | 1401.3    |
| EAGLE_6  | nodes = 2 (16 NUMA regions) | `--map_by numa` | 1294.6     | 1314.2    |
| EAGLE_25 | nodes = 2 (16 NUMA regions) | `--map_by numa` | 8381.4     | 8420.6    |
| EAGLE_50 | nodes = 2 (16 NUMA regions) | `--map_by numa` | 69312.1    | 67273.6   |
| EAGLE_50 | nodes = 4 (32 NUMA regions) | `--map_by numa` | 51803.8    | 51058.3   |
| EAGLE_50 | nodes = 8 (64 NUMA regions) | `--map_by numa` | 41941.1    | 42700.5   |

Notes
----------------

1. Implementing the parallel version [PT-Scotch](https://inria.hal.science/hal-00402893) should improve performance across large node count runs.
2. Further improvement could be achieved by accurately obtaining the cost to communicate across the resources provided by the scheduler at runtime. The above approach using the pre generated `tleaf` file is an approximation and tools like [netloc](https://www.open-mpi.org/projects/netloc/), which derive from the network fabric representive latency values would be the optimal solution. To begin this would require admin access to run the set up commands to generate an overall graph of the whole HPC. This graph structure is then referenced on run time with the allocated nodes ids to build up an accurate reprensentation of the available compute.


